## 5.14. Quiz

### Spark Engine은 무엇을 책임지는가
- **클러스터 전반**에 걸쳐
- **어플리케이션**을 **스케줄링**하고 **배포**하고 **모니터링**하는 책임 존재

### Spark는 데이터를 어떻게 분할하는가
- `MR API`를 사용하여 **데이터 파티셔닝**을 수행
- **입력 형식**에서 **파티션 수**를 생성할 수 있음
- `HDFS 블록 크기`는 최상의 성능을 위한 **파티션 크기**이지만
  - `Split`과 같이 **파티션 크기**를 변경할 수 있음

### 일반적인 Spark 생태계는 무엇인가
- Spark SQL
- Spark Streaming
- 기계 학습을 위한 MLLib
- 그래프 계산을 위한 `GraphX`
- Spark 엔진의 R을 위한 `SparkR`
- 대규모 데이터에 대한 **대화형 쿼리**를 지원하는 `BlinkDB`

### Spark는 데이터를 어떻게 저장하는가
- Spark는 **프로세싱 엔진**
- `HDFS/S3` 및 여러 **스토리지 엔진**에서 데이터 검색 가능

### SparkContext
- Master Driver Program
- 내부 서비스를 설정하고 **Spark 실행 환경**에 연결
- `RDD`를 만들 때, `SparkContext`는
  - Spark Cluster에 연결하여 **새 Spark Context 개체**를 만듦
- `SparkContext`는 `spark cluster`에 access하는 방법을 알려줌

### SparkCore
- Spark FW의 **기본 엔진**
- 메모리 관리, 내결함성, 스케줄링 및 모니터링 작업 제어
- 데이터 저장소와 상호작용

### SparkSQL, HQL, SQL의 차이
- `SparkSQL`은 **구문 변경** 없이 `SQL/HQL`을 지원하는 `SparkCore` 엔진의 **구성 요소**
- `SQL Table/HQL Table`을 조인할 수 있음

### Spark Streaming은 언제 사용하는가
- 스트리밍 데이터의 **실시간 처리 API**
- `웹 서버 로그 파일`, `소셜 미디어`, `주식 시장 데이터`
- `Flume/Kafka`와 같은 **하둡 생태계**와 같은
  - 다양한 소스에서 스트리밍 데이터 수집

### Partition을 변경할 수 없는 이유
- 모든 변환은 **새로운 파티션**을 생성
- **파티션**은 `HDFS API`를 사용하므로
  - **변경 불가능**하고 **분산**되며 **내결함성** 존재

### Spark Mllib
- 데이터 처리르 위해 **클러스터**에서 확장할 수 있는 **알고리즘** 제공
- `Data Scientists`는 이 라이브러리를 **분산 처리**에 사용

### Spark Streaming API의 동작 방식
- 특정 시간대를 구성하고
  - 이 시간동안 수집된 모든 데이터는 **일괄 처리**
- `DStream`(데이터 분리 입력 스트림)
  - 스파크 스트리밍으로 **이동 데이터**를 **micro-batch**처리로 분할하고
  - 스파크 스트리밍 처리를 위해 **스파크 엔진**에 공급
  - API 해당 배치를 코어 엔진에 전달
  - 코어 엔진은 **스티리밍 배치**의 형태로 **최종 결과 생성
  - 출력은 **배치** 형태로 처리를 위해
    - **스트리밍 데이터** 및 **배치 데이터**를 허용

### Broadcast Variables
- **작업**과 함께 **복사본**을 제공하는 대신
  - 각 시스템에 캐시된 **읽기 전용 변수**를 유지
- 두가지 유형의 **공유 변수**
  - Broadcast variable(`e.g. Hadoop 분산 캐시`)
  - 누산기(`e.g. Hadoop counter`)
- 저장된 브로드캐스트 변수를 **작업자 노드**에
  - `RO`값을 보내는 **배열 버퍼**