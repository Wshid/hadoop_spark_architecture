## 6.7. Delta Lake Update, Delete, Time Travel

### Delta Lake Record Update & Delete
- DML 작업 업데이트, 삭제 및 병합 지원
- **변경 데이터 캡처** 및 **GDPR 사용 사례 간소화** 가능
- **파일 수준 단위**로 데이터를 추적하고 수정하기 때문에
  - 전체 파티션, 테이블을 읽고 덮어쓰는 것보다 **효율적**

#### Updates & Time Travel
- **column predicate**가 있는 파일 선택
- **column predicate**를 사용하여 파일을 skip, scan 최적화
- 일치하는 모든 술어를 읽고
  - 새 파일에 레코드를 다시 작성하면
  - 이전 파일이 삭제되지 않고 **삭제 표시 파일**이 됨
- 업데이트가 성공하면
  - `Transaction log`에 `transcation`이 commit
- 이전 버전의 테이블로 돌아가 쿼리하기 위해
  - `Time travel`로 디버깅하는 데
  - **이전 데이터 파일**을 사용

#### Delete & Vacuum
- **열 조건자**에 따라 레코드 선택
- 일치하는 **레코드 파일**을 메모리로 읽고
  - 새 파일을 쓰기 전에 **행을 삭제**
- 삭제 작업이 완료되면
  - 이전 파일이 완전히 삭제되지 않고
  - 디스크에 **삭제 표시 파일**로 남아 있음
- **삭제 취소**를 하려면
  - **Time travel**을 사용하여
  - **이전 버전의 테이블**로 돌아갈 수 있음
- 특정 시간 창에서 **파일 삭제**를 하려면
  - **라이브 테이블**의 일부가 아닌
  - 모든 데이터 파일을 **영구적으로 삭제**하는 `Vacuum` 명령 사용 가능
  - 기본적으로 `7d`로 설정된 **기간 보존 설정**보다 오래된 것을 대상으로 함
  - 이렇게 하려면 `Vacuum` 명령을 수행해야 함 

#### Performance Improvement
- `Z-ordering`을 사용하여 **검색 최적화**를 위해
  - 여러 predicate를 추가하고
- **중복 레코드**를 제거하기 위한 `Bloom filter`를 사용하여 수행

### Update & Delete SQL w/ Time Travel
```sql
-- Update SQL
UPDATE table
SET x=y
WHERE column_predicate condition

-- Time Travel SQL
SELECT * FROM table VERSION AS OF 4

-- Delete SQL
DELETE 
FROM table 
WHERE column_predicate
```

### Update & Delete API w/ Time Travel
```scala
// Update API
table = DeltaTable.forPath(path)
table.update('column_predicate', {'x':'y'})

// Time Travel API
spark.read
  .option('versionAsOf', '4')
  .load(path)

// Delete API
DeltaTable.Vacuum(1000)
```