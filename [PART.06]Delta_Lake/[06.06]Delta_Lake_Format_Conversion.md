## 6.6. Delta Lake Format Conversion

### Delta Lake Format Conversion
- `Delta Table`에서의 기억할 한가지
  - Transaction Log, Schema, 기타 메타데이터, Timestamp, Version 및 데이터 파일이 모두 한 곳
    - 특히 **동일한 디렉터리 계층 구조**내에 존재

#### Open Format
- 모든 `DataFrame`에는
  - 별도의 Metastroe에 DB 스키마, 데이터 유형 및 열과 같은
  - **데이터 형태를 정의**하는 blueprint, metadata가 저장됨
- Delta Lake 테이블을 사용하면
  - schema가 `delta table`이 있는 **동일한 디렉터리**에
  - `transacton log`안 `json` 형식으로 저장
- 별도의 `spark hive db`에 있는 `metadata`가 **병목 현상**을 일으킬 수 있으므로
  - `parquet` 테이블보다 훨씬 효율적이고, 확장 가능
- Parquet 생성과 `Delta table`생성 비교
  ```sql
  -- parquet 생성
  CREATE TABLE foo using parquet

  dataframe
    .write
    .format('parquet')
    .optio('mergeSchema', 'true')
    .save('/tmp')
  
  -- delta 생성
  CREATE TABLE foo using delta
  dataframe
    .write
    .format('delta')
    .option('mergeSchema', 'true')
    .saveAsTable('load_delta')
  ```

### Easy Conversion
- `Spark`에 기존 `parquet` 테이블이 있는 경우
  - 이를 `Delta Lake`형식으로 변환 할 수 있으므로
  - 전체 테이블에 다시 작성할 필요가 없음

### Deletes, Upserts
- `delta lake`에서 `delete/upsert`를 수행하기 위해 `api`를 호출하는 것이 좋음

### Schema Evolution
- 모든 스키마 변경 사항은 `delta table`내에 **저장된 transation json log**에 저장
- 데이터 소스 스키마와 일치하도록
  - 새 열을 자유롭게 추가하고
  - 델타 테이블을 업데이트 할 수 있음