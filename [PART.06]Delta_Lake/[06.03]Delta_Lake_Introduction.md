## 6.3. Delta Lake Introduction
- **QIQO**(Quality in Quality Out)
  - GIGO(Garbage in, Garbage Out)의 반댓말
- **빅 데이터 레이크**에 **Data Warehouse**를 구축하는 핵심은
  - 비즈니스에 대한 **핵심 비즈니스 통찰력**을 만드는데 중요
  - 이후 `시간, 안정성, 성능, 단순성, 상호 작용 가능성/다용도성`에 대해 걱정
- 이는 모두 기존 `Data Warehouse` 및 최신 `Data Lake`에 없는 기여 요인
- `Databricks`는 `Data Warehouse`가 아니라 `Data Lake Warehouse`이기 때문에 `Lake house`라는 용어 사용

### Delta Lake

#### Abstraction
- `Spark`와 `100%` 호환
- 다양한 `Data Lake`에서 실행되므로
  - `Big Data Workflow`에 이미 `Spark`를 사용하는 경우 쉽게 시작 가능
- `SQL, Python, Scala`용 API를 제공
- `ML Model`및 **실험**을 재현하는 것은
  - `Data Scientist`에게 반드시 필요한 기능
    - 이전 모델을 쉽게 **역추적**할 수 있는
    - Prodcution에 투입하기 전 `수백 개`의 모델을 만들고 비교하기 때문
- `Delta Lake` 외부에서 **데이터 관리**는
  - **데이터 과학 도구**와는 분리됨
- `Delta Lake`는 현재 `Databricks`에서 구현한
  - `MLFlow`를 사용하여 `시간 여행`을 적용하여 문제 해결
    - e.g. 모델 교육 작업에 사용된 **데이터 버전**을 추적하기 위해
      - MLFlow **매개변수**로 **경로**에 `Timestamp`가 찍힌 `URL`을 간단히 기록 가능
  - **이전 설정** 및 **데이터세트**로 돌아가 **이전 모델 재현** 가능
  - 데이터에 대해 업스트림 팀과 조정하거나
    - **데이터 복제**에 대해 걱정할 필요가 X
  - **데이터 과학**이 **데이터 엔지니어링**과 밀접하게 결합된 **통합 분석**의 힘

#### Enhancements
- `DML 명령 병합`, `update/delete`를 지원
- `Data lake`의 레코드를 쉽게 `update/insert/delete` 가능
- **변경 데이터 캡처** 및 `GDPR` 사용 사례를 간소화 할 수 있음
- **낙관적 동시성 제어**를 사용하여 **ACID** 트랜잭션 보장
  - 안정적인 데이터 수집에 도움이 됨
- `UPDATE/DELETE/MERGE`를 포함한 `DML` 명령 지원
  - 이 명령은 `Transaction log`를 사용하여 `CDC`(변경 데이터 캡처)를 단순화 하고, 더 정확하게 함
- `Spark Structured Streaming`을 통해
  - 모든 `ETL Batch 작업`을
  - 여러 `app`을 제공할 수 있는 **Streaming ETL 작업**으로 변환 가능
- `Delta Lake` 사용시
  - **스트리밍**과 **일괄** 처리를 동시에 수행하고
  - `CRUD` 작업을 수행하여 **리소스 절약**이 가능