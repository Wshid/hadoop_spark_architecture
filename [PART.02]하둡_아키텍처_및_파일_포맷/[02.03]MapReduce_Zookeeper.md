## 2.3. MapReduce & Zookeeper

### MapReduce
- MR2, Hadoop 2 이상에서 YARN 사용
- 분산 시스템 아키텍처에서 빅데이터를 병렬로 처리
- `share-nothing architecture`
  - 분산 시스템에서 각 `Node`는 다른 `Node`와 완전히 독립적
  - 리소스 병목 현상 방지
  - 동일한 리소스에 대한 서버 경합 방지
    - memory & storage 
  - 개별 Node에 속한 `n`개의 HDFS disk & memory는
    - **클러스터의 다른 Node와 공유 불가**

#### Mapper & Reducer
- 처리할 데이터가 포함된 java process
- 데이터 처리를 **데이터가 포함된 서버**로 이동하는 것이
  - **네트워크**를 통해 데이터를 이동하는 것보다 효율적
- 일반적으로 수행되는 연산
  - parsing, transformation, filtering
- 진행 단계
  - mapper가 입력 데이터 처리
  - 다음 단계인 sort, shuffle에 key,value쌍 출력
  - sort & suffle 단계, **데이터가 정렬되고 분할**
  - 분할되고 정렬된 데이터는
    - 네트워크를 통해 **정렬 및 분할된 데이터**를 읽는
    - `Reducer`로 전송
  - `Reducer` process는 이러한 레코드를 가져오면
    - 여러 작업 수행은 가능하나
    - `HDFS, HBase`와 같은 저장소에 일정량의 데이터를 쓰거나 집계할 가능성이 큼
- 결과적으로, 두 가지 JVM 세트가 있음
  - 데이터 **필터링 및 변환**
  - 데이터 **정렬 및 분할**

### Zookeeper
- **분산 환경**에서는 서비스를 **조정**하고 **관리**하기 어려움
- Node 및 클러스터의 zoo를 관리
- **Cluster Management**
  - 구성 관리
  - 자동 클러스터 Leader 선출
  - 자동 장애 복구
  - 네이밍 서비스
- **Configuration Management**
  - Hbase, Solr
  - Kafka, YARN
- `YARN service registry`
  - zookeeper 위에 구축되어 `cluster management`로도 활용
- `kafka topic`의 구성도 `zookeeper`에서 관리
- zookeeper service 시작시,
  - ensemble에서 한 Node가 `Leader`로 선출
- Client가 쓰기 요청을 하면
  - 연결된 서버는 요청을 `Leader`에게 전달
- 이 `Leader`는 ensemble의 모든 Node에 **동일한 쓰기 요청** 발행
- `write`만 처리하는 leader node
- `read`만 처리하는 follower
- `leader`가 다운되면
  - 순서 번호가 가장 작은 `follower`를 다음 `leader`로 자동 선택
- `zookeeper`를 실행하기 위해
  - 최소 **3개**의 서버가 필요한 quorum(정족수) 개념 사용
- `zookeeper`는 `memory db`에서 데이터를 복제, 저장하기 위해 사용
- 모든 `Node`는 `read/write`요청을 위한 자체 `memory db`가 존재
- 데이터가 fs에 성공적으로 기록될 때까지
  - inmemory db에 `commit`되지 X
- `Request Processor`
  - Leaer Node에서 쓰기 요청을 처리할 때 사용
- `Atomic Broadcast`
  - `Leader`는 변경 사항을 `Follower`로 `broadcast`하고
  - `Follower`는 변경 사항을 `Leader`에게 알림